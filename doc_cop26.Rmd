---
title: "Sentiment analysis of COP26-related headlines from a British outlet"
subtitle: "Quantitative Text Analysis of 'The Guardian' headlines"
author: "Katalin Bayer, Laura Menicacci, Nassim Zoueini"
date: "19/12/2021"
output: 
  html_document:
    toc: true
    toc_collapsed: true
    toc_float: true
    toc_depth: 3
    code_folding: show
    number_sections: false
    theme: lumen
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## Executive summary


we target the headlines of "The Guardian" newspaper in the period going from 18 October 2021 and 28 November 2021 to apply a sentiment analysis around the negotiations of COP26 that took place in Glasgow from October 31st and November 12. 


## Statement of contributions 


## Introduction 
The 26th Conference of the Parties that took place in Glasgow has represented a crucial moment for climate policy negotiations. 
The majority of the most influential international leaders attended the event to discuss on future global action regarding climate mitigation and adaptation, together with non-state actors and internationally renowned personalities. Such occasion gained substantial media attention from all over the world, with peaks in the intervals right before the starting of the COP (the so-called 'PreCOP' events), during the actual happening of the Conference, and right after the conclusion of such event. 
However, media outlets would approach climate change in different ways that reflect their political positioning: the headlines, the highlights as well as the frequently mentioned topics would differ based on political position. 
We collect data from the headlines of a British newspaper to analyse possible trends and changes in sentiment along the specific timeframe that goes from the weeks right before the Conference until the period right after it. 

## Motivation
As public policy students concerned about climate negotiations, we are interested in investigating the different opinions and attitudes expressed by media outlets in the above-mentioned periods of time. We are mainly concerned on whether the standpoint and the perspective of media outlets changed over time, and how the trends of this change would be developed. 
To accomplish that, we decided to analyse the sentiment of just one media outlet published in the COP26 host country, namely The Guardian. Moreover, this newspaper is considered as a left-leaning, according to [YouGov](https://yougov.co.uk/topics/politics/articles-reports/2017/03/07/how-left-or-right-wing-are-uks-newspapers) findings. Topics as climate financing for instance were a crucial topic on the table of COP26 negotiations, hence choosing a non-neutral outlet - which would have endorsed such topic - can show more compelling results in terms of changes in the political positioning with respect to the outcomes, that will eventually be reflected in the headlines. 
 

## Research question
The principal objective of this research aims at analysing trends in the attitude of the newspaper headlines with respect to COP26 topics. Then, the other questions related to this analysis are related to two macro areas. 
The first one tackles the original and main interest, that is whether the ratio of positive and negative words changes over time, and in which period this eventually happens. Questions related to this area are: 
 - Are the words used in the headlines of 'The Guardian' providing a specific sentiment? 
 - What is the ratio of positive over negative words in the collected data? Does this ratio changes over time? 
 - Overall, is this sentiment more positive or negative? Which are the most positive and negative words? 
 - Are there any possible interesting patterns among the most frequent words that could be inspected further? 
The second area concerns a more specific analysis that also takes into account how such results could change when using different measurement instruments, in this case, dictionaries. Questions related to this area are: 
 - Is the sentiment analysis consistent across different dictionaries?
 - Do any differences and/or overlaps induce any political interpretation? Are the results relevant for political interpretation? 
 

## Limitations 
Being a newspaper of the host country of the climate negotiations, The Guardian would not represent an ideal sample of headlines that would allow us to deduce if COP26 has met the expectations or not through the sentiment analysis. Indeed, the results would only show the changes in opinions for the specific political leaning that such outlet represents. However, the values of this project are to apply procedures of sentiment analysis after scraping information from the web and present them to the user in an accessible format. Therefore, it is necessary to acknowledge the very limited scope of this analysis. 
Another present limitation regards the dates that have been scraped from The Guardian website. Given the used web-scraping strategy, the most recent dates present some missing values caused by an heterogeneous format in the website pages. For demonstration purposes we simply dropped those missing values. 



## QUESTION 1

```{r, include=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, 
               rvest, 
               stringr, 
               xml2, 
               quanteda, 
               janitor, 
               tidytext, 
               lubridate, 
               wordcloud, 
               SnowballC, 
               dygraphs, 
               xts, 
               tsbox)
```
<br>

<h2>
2. Scraping Headlines by Date and Data Cleaning
</h2>
<div>
<i> To examine this step in details, check the attached scraping.R file on <a href="url"> https://github.com/lauramenicacci/cop26_text_analysis/blob/e3697a69534a5e9d540bfc1913f52bd096de99ee/scraping.R</a> </i>
</div>
```{r}
source("./scraping_and_data_cleaning.R")

```

<br>
<br>
<h2>
3. Explorative analysis
</h2>
<div class="comment">
Through the wordcloud, we observed that words as "cop26", "glasgow","climate","change" were very frequent and not really useful for a sentiment analysis, so we removed them.
PLEASE LAURA ADD OTHER RELEVANT DESCRIPTION HERE
3.1. Frequency Table 
<br>
3.2. Explorative Wordcloud
<br>
3.3.Keyword in Context
<br>
</div>
<br>
<br>
<h3>
3.1. Frequency Table
</h3>
```{r}
words_list %>% 
  count(word, sort = TRUE) %>% 
  head(10)
 

#frequency plot + take most frequent words and see where they appear WE DROPPED THIS?

```
<br>
<h3>
3.2. Explorative Wordcloud
</h3>
```{r}
#explorative wordcloud: we see the most frequent words: if there's something useless we remove it 
wordcloud(words_list$word, min.freq = 20, random.order = FALSE, colors=brewer.pal(8, "Dark2"))
```
<div class="comment">
<i> Exploring the most frequent words helps to remove the irrelevant ones among them by adding them to the stopwords dictionary i.e. Glasgow. EDIT PLEASE </i>
<br>
</div>
<div>
<i>Check scraping file https://github.com/lauramenicacci/cop26_text_analysis/blob/e3697a69534a5e9d540bfc1913f52bd096de99ee/scraping.R</a>
</i>
<br>
</div>
<br>
<h3>
3.3. Keyword in Context
</h3>
```{r}

cop26$Headlines %>% 
  corpus() %>% 
  tokens() %>% 
  kwic(phrase("crisis"), window = 10) %>% 
  DT::datatable(caption = "Keywords in context", rownames = FALSE, options = list(scrollX = TRUE, pageLength = 5, lengthMenu = c(5, 10, 15, 20)))
```
<br>
<br>
<h2>
4. Sentiment analysis Using "bing" Dictionary
</h2>
<br>
<div class="comment">
Result: we have a dataframe with words, dates, sentiment and frequency of occurrence of those words
first plot: we start from the general: visualize through time how sentiment changes! TO BE EDITED

<br>
4.1. Sentiment Analysis Dataframe
<br>
4.2. Basic Visualization
</div>
<br>
<h3>
4.1. Sentiment Analysis Dataframe
</h3>
```{r}
word_sent <- words_list %>% #we have 293 classified words
  filter(!word == "crisis") %>% #remove the word crisis before the sentiment analysis
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment) 
   
sentiment_complete <- left_join(words_list, word_sent, by = "word") #join with the main df

#word_sent %>% 
#  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) if we want to flip it 



#add breaks is this DONE (?)
#add interactive slider for dates DONE
#change color



```
<br>
<h3>
4.2. Basic Visualization
</h3>

```{r}
sc <- sentiment_complete %>% #first plot of sentiment analysis
  drop_na()

sc$n[sc$sentiment == 'negative'] = -sc$n 


sc %>% 
    ggplot(aes(x = Dates, y = n, fill= sentiment))+
    geom_bar(position="dodge", stat="identity", width= 0.5) +
    labs(title = "Sentiment frequency plot by date", y = "Count", x = "Date", fill = "sentiment") +
    theme_minimal()
```

<br>
<br>

<h2>
5. Exploration of Results
</h2>
<br>
<div class="comment">
PLEASE LAURA ADD DESCRIPTION HERE

we try to understand which are the most used words overall. First we plot them without dates to see the overall result, then we add them in a second plot
<br>
5.1. Plot of Results
<br>
5.2. Summary Interactive Plot by Date
<br>
5.2.1. Manipulation of the Dataset into a #timeseries' Object
<br>
5.2.2. All Sentiment Interactive Plot by Date
<br>
5.2.3. Positive vs Negative Sentiment Interactive Plots by Date
</div>
<br>
<h3>
5.1. Plot of Results
</h3>
```{r}
sc %>% 
  drop_na() %>% 
  count(word, sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>%
  filter(n > 4) %>% 
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```
<br>
<h3>
5.2. Summary Interactive Plots
</h3>
<h4>
5.2.1. Manipulation of the Dataset into a 'timeseries' Object
</h4>
```{r}

sentiment_complete %>%
  count(Dates,sentiment) %>% 
  mutate(across(sentiment, .fns = ~replace_na(.,"neutral")))-> counter #creating a dataframe counting each type of sentiment, replacing NA sentiment by neutral

counter %>% 
  build_wider_spec(names_from = sentiment, values_from = c(n))->spec1
counter<-counter %>% 
  pivot_wider_spec(spec1) %>% 
  na.omit() # to pivot the data frame, single date per line, all sentiments per column 

negatives<-xts(counter$negative, counter$Dates) #creating a time series for each sentiment
positives<-xts(counter$positive, counter$Dates)
neutrals<-xts(counter$neutral, counter$Dates)

timeseries <- cbind(negatives, positives,neutrals) # binding the three time series

```
<br>
<h4>
5.2.2. All Sentiment Interactive Plot by Date
</h4>
```{r}
dygraph(timeseries) %>%  #the plot, unfortunately, the dates are lost so far
  dySeries("negatives", label="Negative") %>% 
  dySeries("positives", label="Positive") %>% 
  dySeries("neutrals", label="Neutral") %>%
  dyOptions(labelsUTC = TRUE, fillGraph=TRUE, fillAlpha=0.5, drawGrid = FALSE) %>%
  dyRangeSelector() %>%
  dyCrosshair(direction = "vertical") %>%
  dyHighlight(highlightCircleSize = 3, highlightSeriesBackgroundAlpha = 0.3, hideOnMouseOut = FALSE)  %>%
  dyRoller(rollPeriod = 1) 
```
<br>
<h4>
5.2.3. Positive vs Negative Sentiment Interactive Plots by Date
</h4>
```{r}

timeseries2<-cbind(negatives, positives)
dygraph(timeseries2) %>%  #the plot, unfortunately, the dates are lost so far
  dySeries("negatives", label="Negative") %>% 
  dySeries("positives", label="Positive") %>% 
  dyOptions(labelsUTC = TRUE, fillGraph=TRUE, fillAlpha=0.5, drawGrid = FALSE) %>%
  dyRangeSelector() %>%
  dyCrosshair(direction = "vertical") %>%
  dyHighlight(highlightCircleSize = 3, highlightSeriesBackgroundAlpha = 0.3, hideOnMouseOut = FALSE)  %>%
  dyRoller(rollPeriod = 1)
```

<br>
<br>
<h2>
6. Targeted sentiment analysis
</h2>
<div class="comment">
KATALIN PLEASE EDIT HERE
<br>
6.1 Sentiment Density for COP26
<br>
6.2 Sentiment Density, Target: Boris Johnson
</div>
<br>
<h3>
6.1 Sentiment Density for COP26
</h3>
```{r}
c26_density <- cop26$Headlines %>% 
  corpus() %>% 
  tokens()

sent_c26 <- tokens_lookup(c26_density, dictionary = data_dictionary_LSD2015[1:2]) %>% 
  dfm() %>% 
  convert(to = "data.frame") #targeted sentiment analysis for the boris words


sent_c26$score <- log((sent_c26$positive + 0.5) / (sent_c26$negative + 0.5))

sent_c26$average

sent_c26 %>% 
  ggplot(aes(y = score)) + 
  geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.6) +
  coord_flip() +
  theme_minimal()

```
<br>
<h3>
6.2 Sentiment Density, Target: Boris Johnson
</h3>
```{r}

toks_cop26 <- cop26 %>% 
 filter(str_detect(Headlines, "Boris Johnson")|str_detect(Headlines, "Boris") | str_detect(Headlines, "Johnson"))
  
boris_c26 <- toks_cop26$Headlines %>% 
  corpus() %>% 
  tokens()

sent_boris <- tokens_lookup(boris_c26, dictionary = data_dictionary_LSD2015[1:2]) %>% 
  dfm() %>% 
  convert(to = "data.frame") #targeted sentiment analysis for the boris words


sent_boris$score <- log((sent_boris$positive + 0.5) / (sent_boris$negative + 0.5))

sent_boris$average

sent_boris %>% 
  ggplot(aes(y = score)) + 
  geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.6) +
  coord_flip() +
  theme_minimal()
```

```{r}
# I kept these but we don't need them, 
#sent_negative <- words_list %>% #we have 293 classified words, DO WE NEED THESE? THE NEGATIVES ONLY
 # filter(!word == "crisis") %>% #remove the word crisis before the sentiment analysis
  #inner_join(get_sentiments("bing")) %>% 
  #filter(sentiment=="negative")

#sent_negative %>% 
 # count(Dates, sentiment) %>% 
  #ggplot(aes(x=Dates, y=n)) +
  #geom_line(color="red") +
  #theme_minimal() +
  #ylab("Frequency of Negative Words during COP26")+
  #xlab("Date")




#I kept the ggplot on case we will drop the interactive ones
#sentiment_complete %>% 
 # drop_na() %>% 
  #count(Dates, sentiment) %>% 
  #group_by(sentiment) %>%
  #ggplot(aes(Dates,y=n,color=sentiment)) +
  #geom_line() +
  #facet_wrap(~ sentiment, scales = "free_x") +
  #theme_minimal() +
  #ylab("Frequency of Negative Words during COP26")+
  #xlab("Date")

```


```{r}
#FOR OTHER IDEAS

#https://www.tidytextmining.com/ngrams.html
```


