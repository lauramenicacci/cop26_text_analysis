---
title: "Sentiment analysis of COP26-related headlines from a British outlet"
author: "Katalin Bayer, Laura Menicacci, Nassim Zoueini"
date: "19/12/2021"
output: 
  html_document:
    fig_width: 6
    fig_height: 4
    toc: true
    toc_collapsed: true
    toc_float: true
    toc_depth: 3
    code_folding: show
    number_sections: false
    theme: lumen
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## Quantitative Text Analysis of 'The Guardian' headlines

### Executive summary
The following report presents a quantitative text analysis of "The Guardian" newspaper's headlines in the periods going from 18 October 2021 and 28 November 2021 to apply a sentiment analysis around the negotiations of COP26 that took place in Glasgow from October 31st and November 12th. Such descriptive analysis aims at finding eventual changes in the opinions and/or political positioning of the abovementioned media outlet. 
ADD CONCLUSIONS 

### Statement of contributions 
 - Laura
 - Kat
 - Nassim

```{r, echo = FALSE}
# Loading/installing all packages required for this analysis
if (!require("pacman")) install.packages("pacman")
#webshot::install_phantomjs()
pacman::p_load(tidyverse, 
               rvest, 
               stringr, 
               xml2, 
               quanteda, 
               janitor, 
               tidytext, 
               lubridate, 
               wordcloud, 
               SnowballC, 
               dygraphs, 
               xts, 
               tsbox,
               tm, 
               kableExtra,
               magick,
               textdata, 
               plotly, 
               htmlwidgets, 
               htmltools)

```


### Introduction 
The 26th Conference of the Parties that took place in Glasgow has represented a crucial moment for climate policy negotiations.  

The majority of the most influential international leaders attended the event to discuss on future global action regarding climate mitigation and adaptation, together with non-state actors and internationally renowned personalities. Such occasion gained substantial media attention from all over the world, with peaks in the intervals right before the starting of the COP (the so-called 'PreCOP' events), during the actual happening of the Conference, and right after the conclusion of such event.  

However, media outlets would approach climate change in different ways that reflect their political positioning: the headlines, the highlights as well as the frequently mentioned topics would differ based on political position.  

We collect data from the headlines of a British newspaper to analyse possible trends and changes in sentiment along the specific timeframe that goes from the weeks right before the Conference until the period right after it.  

### Motivation
As public policy students concerned about climate negotiations, we are interested in investigating the opinions and attitudes expressed by media outlets in the above-mentioned periods of time. We are mainly concerned on whether the standpoint and the perspective of media outlets changed over time, and how the trends of this change could have developed.   

The relevance of our analysis stands in our curiosity for newspapersâ€™ behavior concerning international, critical occasions as the COP26. Understanding whether they actually aim to inform people with the addition of a particular sentiment (that could also aim at reflecting a general feeling from the readers), or whether they prefer to remain neutral and objectively report factual events, could foster a deeper comprehension of the role of information and media in climate change developments.  

To accomplish that, we decided to analyse the sentiment of just one media outlet that is published in the COP26 host country, UK, that is The Guardian. This newspaper is considered as a left-leaning, according to [YouGov](https://yougov.co.uk/topics/politics/articles-reports/2017/03/07/how-left-or-right-wing-are-uks-newspapers) findings. For instance, topics as climate financing were among the most critical ones put on the table of COP26 negotiations, hence choosing a non-neutral outlet - which would have endorsed such topic - can show more compelling results in terms of changes in the political positioning with respect to the outcomes of the Conference, that can eventually be reflected in the headlines.   
 

### Research question
The principal objective of this research aims at analysing trends in the attitude of the newspaper headlines with respect to COP26 topics. Then, the other questions related to this analysis are related to two macro areas. 
The first one tackles the original and main interest, that is whether the ratio of positive and negative words changes over time, and in which period this eventually happens. Questions related to this area are: 

 - Are the words used in the headlines of 'The Guardian' providing a specific sentiment? 
 - What is the ratio of positive over negative words in the collected data? Does this ratio changes over time? 
 - Overall, is this sentiment more positive or negative? Which are the most positive and negative words? 
 - Are there any possible interesting patterns among the most frequent words that could be inspected further? 

The second area concerns a more specific analysis that also takes into account how such results could change when using different measurement instruments, in this case, dictionaries. Questions related to this area are: 

 - Is the sentiment analysis consistent across different dictionaries?
 - Do any differences and/or overlaps induce any political interpretation? Are the results relevant for political interpretation? 

### Methods
The text analysis is performed mainly using two different packages: tidytext and quanteda. Both packages follow the tidyverse design philosophy. The main difference between these two tools is that quanteda works with Corpus objects, proper of the NLP logic, while tidytext can process texts in their character format. We employed both tools to carry on all of our research questions in the most appropriate way. Specifically, tidytext was useful to build analyses and visualizations with dates, in a simpler manner than with the quanteda document level variables. The quanteda package was instead particularly useful for the targeted sentiment analysis we conducted, together with the fact that it was possible to check the consistency of results also with another dictionary, the LSD2015 one. The keywords in context function was also used as an explorative tool. 

### Limitations 
Being a newspaper of the host country of the climate negotiations, The Guardian would not represent an ideal sample of headlines that would allow us to deduce if COP26 has met the expectations or not through the sentiment analysis. Indeed, the results would only show the changes in opinions for the specific political leaning that such outlet represents. However, the values of this project are to apply procedures of sentiment analysis after scraping information from the web and present them to the user in an accessible format. Therefore, it is necessary to acknowledge the very limited scope of this analysis. The relevance of such investigation can only be applied to this specific and small sample.   

Additionally, a further limitation concerns the dates that have been scraped from The Guardian website. Given the used web-scraping strategy, the most recent dates (December and end of November 2021) present some missing values caused by a heterogeneous format in the website pages. For demonstration purposes we simply dropped those missing values, further limiting the scope of the analysis.  

## Retrieving the data
The webscraping, cleaning and formatting section of the analysis can be found in the R script *scraping_and_data_cleaning* that is available in the repository. 
```{r, echo = FALSE}

source("./scraping_and_data_cleaning.R")

```
The webscraping strategy adopted consists in downloading the headlines from multiple pages of the newspaper website by  date (static webscraping). 
The formatting step includes transformation of dates into the correct format with lubridate and and data preparation for the quantitative text analysis with tidytext. 
In this part, words regarding the main topic of the headlines ("cop26", "glasgow","climate","change") were expected to be very frequent, other than not contributing to a specific senitment, so they have been removed as stopwords. 

## Explorative analysis 
Through the exploration of the collected data, we aim at understanding which are the most frequent words and whether they could have a role in our investigation. 

Thanks to a frequency table and an explorative WordCloud, we visualize the most frequent words. We identify 'crisis' as the most frequent word (other than the customized stopwords) used in the headlines during the COP26 period. 

Thanks to the keyword in context table, it is explored quickly whether any case in which the word 'crisis' has a role different from being part of the 'climate crisis' bigram is present. It is not found to be the case. Since the main topic of COP26 is exactly that of 'tackling the climate crisis', this word, despite clearly indicating a negative sentiment, does not represent relevant information. It is therefore dropped.  

### WordCloud

Comment on wordcloud


```{r, echo = FALSE}
#explorative wordcloud: we see the most frequent words: if there's something useless we remove it 

par(mar=c(0.1,0.1,0.1,0.1))

wordcloud(words_list$word, 
          min.freq = 20,
          random.order = FALSE, 
          colors=brewer.pal(8, "Dark2"))

```

```{r, echo = FALSE}
frequencytable <- words_list %>% 
      count(word, sort = TRUE) %>% 
      head(10) %>% 
      kbl() %>% 
      kable_styling(bootstrap_options = c("hover", "condensed", "responsive"), full_width = F, fixed_thead = T)
frequencytable
save_kable(frequencytable, "./visualization/3_2FrequencyTable.html")
      

```

### Keywords in context for the most frequent word
```{r, echo = FALSE}
keywordcontext <- cop26$Headlines %>% 
      corpus() %>% 
      tokens() %>% 
      kwic(phrase("crisis"), window = 10) %>% 
      DT::datatable(caption = "Keywords in context", rownames = FALSE, options = list(scrollX = TRUE, pageLength = 5, lengthMenu = c(5, 10, 15, 20))) 

keywordcontext
  htmlwidgets::saveWidget(keywordcontext, "./visualization/3_3KeywordsInContext.html")
```

## Sentiment analysis 
The sentiment analysis applied to the collected headlines is conducted using a dictionary-based method. The three used dictionaries are:  

 - â€˜Bing et Al.â€™,  

 - â€˜AFINNâ€™  

 - â€˜Lexicoder Sentiment Dictionaryâ€™ (LSD2015) 

The choice of these dictionaries is mainly based on common practice and on the objective of our research to check the sentiment of the headlines around the climate negotiations, quantify them and detect any potential patterns and the consistency of these results. 

From the tidytext package, we use the â€˜Bing et al.â€™  and the â€˜AFINNâ€™ dictionaries. These are general-purpose lexicons based on unigrams (single words). The first one classifies the words into negative or positive, while the second one scales the sentiment by assigning a value between a range of -5 and +5, classifying words with values very negative and very positive respectively.  

From the quanteda package, the Lexicoder Sentiment Dictionary represents a more than valid alternative, due to its particular versatility with respect to sentiment analysis for political communication (Young, L. & Soroka, S., 2012). Such dictionary consists of 2,858 'negative' sentiment words and 1,709 'positive' sentiment words. The novelty of Young and Soroka approach stands in a further set of 2,860 and 1,721 negations of negative and positive words, respectively. However, we did not find such additional set useful for our research purposes.

### Bing et al. visualization
```{r, echo = FALSE}
words_sentiments <- words_list %>% #we have 293 classified words
  filter(!word == "crisis") %>% #remove the word crisis before the sentiment analysis
      inner_join(get_sentiments("bing")) %>% 
      count(word, sentiment) 

sentiment_complete <- left_join(words_list, words_sentiments, by = "word") #join with the main df
    
toks_cop26 <- cop26 %>% 
  filter(str_detect(Headlines, "Boris Johnson")|str_detect(Headlines, "Boris") | str_detect(Headlines, "Johnson"))
    
words_list_boris<- toks_cop26 %>% 
  unnest_tokens(word, Headlines, token = "regex", pattern = "\\s+|[[:punct:]]+") %>%
  anti_join(customized_stopwords) %>% 
  arrange(Dates) %>%
  count(word, sort = TRUE)

```

#### Sentiment frequency
```{r, echo = FALSE}
sentiment_complete <- sentiment_complete %>% 
  mutate(across(sentiment, .fns = ~replace_na(.,"neutral"))) %>%
  mutate(across(n, .fns = ~replace_na(.,0))) 

#first plot of sentiment analysis
sentiment_complete$n[sentiment_complete$sentiment == 'negative'] <-  -sentiment_complete$n[sentiment_complete$sentiment == 'negative']


sentiment_complete %>% 
  filter(sentiment != "neutral") %>%
  ggplot(aes(x = Dates, 
             y = n, 
             fill = sentiment))+
  geom_bar(stat = "identity", 
           width = 0.5) +
  labs(title = "Sentiment frequency plot by date", 
       y = "Count", 
       x = "Date", 
       fill = "sentiment") + 
  geom_vline(xintercept= as.Date("2021-10-31"), 
             linetype = "dashed", 
             colour = "darkgrey") +
  geom_vline(xintercept= as.Date("2021-11-12"), 
             linetype = "dashed", 
             colour = "darkgrey") +
  theme_minimal() + 
  theme(panel.border = element_rect(color = "black",
                                    fill = NA,
                                    size = 0.3),
                         plot.background = element_rect(colour ="white"))

ggsave("./visualization/4_2SentimentFrequencyPlot.png", width = 5, height = 3)
```

#### Most Frequently occuring Words per sentiment
```{r, echo = FALSE}
sentiment_complete %>% filter(sentiment != "neutral") %>%
  #drop_na() %>% 
  count(word, sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>%
  filter(n > 4) %>% 
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ sentiment, scales = "free_y") +
  labs(title = "Most frequently occuring words that affect sentiments", x = "Frequency", y = NULL)+
  theme_minimal() + 
  theme(panel.border = element_rect(color = "black",
                                    fill = NA,
                                    size = 0.3),
        plot.background = element_rect(colour ="white"))

ggsave("./visualization/5_1MostFrequentlyOccuringWord.png", width = 5, height = 3)
```

```{r, echo = FALSE}
# Manipulation of the Dataset into a 'timeseries' Object

sentiment_complete %>%
  count(Dates,sentiment) %>% 
  mutate(across(sentiment, .fns = ~replace_na(.,"neutral")))-> counter #creating a dataframe counting each type of sentiment, replacing NA sentiment by neutral

counter %>% 
  build_wider_spec(names_from = sentiment, values_from = c(n))->spec1
counter<-counter %>% 
  pivot_wider_spec(spec1) %>% 
  na.omit() # to pivot the data frame, single date per line, all sentiments per column 

negatives<-xts(counter$negative, counter$Dates) #creating a time series for each sentiment
positives<-xts(counter$positive, counter$Dates)
neutrals<-xts(counter$neutral, counter$Dates)

timeseries <- cbind(negatives, positives, neutrals) # binding the three time series

```

#### All Sentiment Interactive Plot by Date
```{r, echo = FALSE}
sentiment_timeseries <- dygraph(timeseries, 
                                main = "All sentiments across time", 
                                xlab = "Date", 
                                ylab = "Frequency") %>%
  dySeries("negatives", label="Negative") %>% 
  dySeries("positives", label="Positive") %>% 
  dySeries("neutrals", label="Neutral") %>%
  dyOptions(labelsUTC = TRUE, 
            fillGraph=TRUE, 
            fillAlpha=0.5, 
            drawGrid = FALSE) %>%
  dyRangeSelector() %>%
  dyCrosshair(direction = "vertical") %>%
  dyHighlight(highlightCircleSize = 3, 
              highlightSeriesBackgroundAlpha = 0.3, 
              hideOnMouseOut = FALSE)  %>%
  dyRoller(rollPeriod = 1) 

sentiment_timeseries

save_html(sentiment_timeseries, "./visualization/5_2_2TimeSeriesofSentiment.html", libdir = "lib")

```

#### Positive vs Negative Sentiment Interactive Plots by Date
```{r, echo = FALSE}

timeseries2<-cbind(negatives, positives)
posnegtimeseries <- dygraph(timeseries2, 
                            main = "Positive and negative sentiments across time", 
                            xlab = "Date", 
                            ylab = "Frequency") %>% 
  dySeries("negatives", label="Negative") %>% 
  dySeries("positives", label="Positive") %>%
  dyOptions(labelsUTC = TRUE, 
            fillGraph=TRUE, 
            fillAlpha=0.5, 
            drawGrid = FALSE) %>%
  dyRangeSelector() %>%
  dyCrosshair(direction = "vertical") %>%
  dyHighlight(highlightCircleSize = 3, 
              highlightSeriesBackgroundAlpha = 0.3, 
              hideOnMouseOut = FALSE)  %>%
  dyRoller(rollPeriod = 1)

posnegtimeseries

save_html(posnegtimeseries, "./visualization/5_2_3TimeSeriesofPosNegSentiment.html", libdir = "lib")
```

### AFINN Visualization

#### AFINN analysis for all headlines
```{r, echo = FALSE}
#AFINN Analysis
word_sent_afinn <- words_list %>% #we have 293 classified words
  filter(!word == "crisis") %>% #remove the word crisis before the sentiment analysis
  inner_join(get_sentiments("afinn")) %>% # to load the AFINN dataset, you have to give permission from the console if you never uploaded it yet
  count(word, value) 
   

sentiment_complete_afinn <- left_join(words_list, word_sent_afinn, by = "word") #join with the main df

sentiment_complete_afinn <- sentiment_complete_afinn %>%
  count(Dates,value) %>% 
  mutate(across(value, .fns = ~replace_na(.,"0")))

sentiment_complete_afinn$value<-as.numeric(sentiment_complete_afinn$value)

sentiment_complete_afinn <- sentiment_complete_afinn %>% 
  na.omit() %>%
  filter(value!="0")

sentiment_complete_afinn$value <- factor(sentiment_complete_afinn$value, levels= c("-5","-4","-3","-2","-1","1","2","3","4","5"))

colours <- c( "#b2182b","#d6604d", "#f4a582","#fddbc7","#d1e5f0","#92c5de","#4393c3","#2166ac")
#copying the colors, using https://imagecolorpicker.com/
scatter_result <- plot_ly(sentiment_complete_afinn, 
                         x = ~Dates, 
                         y = ~value, 
                         color = ~value, 
                         size = ~n, 
                         colors = colours, 
                         sizes = c(10,40),
                         type = 'scatter', 
                         mode = 'markers',
                         marker = list(symbol = 'circle', 
                                       sizemode = 'diameter',
                                       line = list(width = 1, color = 'lightgrey')),
                         text = ~paste('Count of Words', n), 
                         alpha = 1.2, #setting interactions
                         width = 750, 
                         height = 450) 


scatter_result <- scatter_result %>% layout(title = 'AFINN Dictionary Results',
         xaxis = list(title = 'Dates',
                      gridcolor = 'white',
                      range=c("2021-10-14","2021-11-20"),
                      zerolinewidth = 1,
                      ticklen = 3,
                      gridwidth = 2),
         yaxis = list(title = 'Sentiment Value',
                      gridcolor = 'lightgray',
                    
                      zerolinewidth = 1,
                      ticklen = 3,
                      gridwith = 2),
         paper_bgcolor = 'rgb(255, 255, 255)',
         plot_bgcolor = 'rgb(255,255,255)')

scatter_result

htmlwidgets::saveWidget(partial_bundle(scatter_result), "./visualization/6_2AffineAnalysisC26.html")

```

#### AFINN analysis for Boris Johnson headlines

```{r, echo = FALSE}
#Afinn Analysis Boris
word_sent_afinn_boris <- words_list_boris %>% 
  filter(!word == "crisis") %>% #remove the word crisis before the sentiment analysis
  inner_join(get_sentiments("afinn")) %>% 
  count(word, value) 
   
sentiment_boris_afinn <- left_join(words_list, word_sent_afinn_boris, by = "word") #join with the main df

counter_boris_afinn <- sentiment_boris_afinn %>%
  count(Dates,value) %>% 
  mutate(across(value, .fns = ~replace_na(.,"0")))

# Structuring the order of the values so that they appear in the correct order in the legend
counter_boris_afinn$value<-as.numeric(counter_boris_afinn$value)
counter_boris_afinn<-counter_boris_afinn %>% 
  #na.omit() %>%
  filter(value!="0") 
counter_boris_afinn$value<- factor(counter_boris_afinn$value, levels= c("-5","-4","-3","-2","-1","1","2","3","4","5"))

colours <- c( "#b2182b","#d6604d", "#f4a582","#fddbc7","#d1e5f0","#92c5de","#4393c3","#2166ac")
#copying the colors, using https://imagecolorpicker.com/

scatter_result_boris<- plot_ly(counter_boris_afinn, 
                               x = ~Dates, 
                               y = ~value, 
                               color = ~value, 
                               size = ~n, 
                               colors = colours,
                               sizes = c(10,40),
                               type = 'scatter', 
                               mode = 'markers',
                               marker = list(symbol = 'circle', 
                                             sizemode = 'diameter',
                                             line = list(width = 1, color = 'lightgrey')),
                               text = ~paste('Count of Words', n), 
                               alpha = 1.2,
                               width = 750, 
                               height = 450) #setting interactions


scatter_result_boris<-scatter_result_boris %>% layout(title = 'AFINN Dictionary Results for Boris Headlines',
         xaxis = list(title = 'Dates',
                      gridcolor = 'white',
                      range=c("2021-10-14","2021-11-20"),
                      zerolinewidth = 1,
                      ticklen = 3,
                      gridwidth = 2),
         yaxis = list(title = 'Sentiment Value',
                      gridcolor = 'lightgray',
                      zerolinewidth = 1,
                      ticklen = 3,
                      gridwith = 2),
         paper_bgcolor = 'rgb(255, 255, 255)',
         plot_bgcolor = 'rgb(255,255,255)')
scatter_result_boris

htmlwidgets::saveWidget(partial_bundle(scatter_result_boris), "./visualization/6_3AffineAnalysisBoris.html")


```

### LSD2015 Visualization

#### Distribution of Sentiments
```{r, echo = FALSE}

c26_tokens <- cop26$Headlines %>% 
  corpus() %>% 
  tokens()
boris_tokens <- toks_cop26$Headlines %>% 
  corpus() %>% 
  tokens()

c26_sent <- tokens_lookup(c26_tokens, dictionary = data_dictionary_LSD2015[1:2]) %>% 
  dfm() %>% 
  convert(to = "data.frame") %>% #targeted sentiment analysis for the cop26 tokens
  mutate(Type = "COP26")
  
boris_sent <- tokens_lookup(boris_tokens, dictionary = data_dictionary_LSD2015[1:2]) %>% 
  dfm() %>% 
  convert(to = "data.frame") %>% #targeted sentiment analysis for the boris tokens
  mutate(Type = "'Boris Johnson'")

sentiment <- rbind(c26_sent,boris_sent)
sentiment$score <- log((sentiment$positive + 0.5) / (sentiment$negative + 0.5))

sentiment %>% 
  ggplot(aes(x = score, fill = Type)) + 
  geom_density(alpha=0.5) +
  labs(title = "Distribution of sentiments",
       x = "Sentiment Score", 
       y = "Density")+
  scale_fill_brewer(type = "qual", palette = "Accent") +
  theme_minimal() + 
  theme(panel.border = element_rect(color = "black",
                                    fill = NA,
                                    size = 0.3),
        plot.background = element_rect(colour ="white"))

ggsave("./visualization/6_1DensityPlots.png", width = 5, height = 3)

```

## Concluding remarks

## Further research suggestions

## Resources

