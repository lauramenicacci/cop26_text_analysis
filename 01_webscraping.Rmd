---
title: "01_downloading articles"
author: "Laura Menicacci"
date: "28/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages

```{r}
library(tidyverse)
library(rvest)
library(stringr)
library(xml2)
library(quanteda)
library(janitor)
library(tidytext)
library(lubridate)
library(wordcloud)
library(SnowballC)
library(dygraphs)
library(xts) 
library(tsbox)
```
<br>

## Part 1 - Scraping headlines by date 
```{r}
source("./scraping.R")

```
what i did until now: 
 - create a list of all guardian pages within the chosen period (18 octr to 28 nov) (RMK:first/last page is with dates also before after 28 nov/18 oct but we can remove them in the cleaning part)
 - try to download pages from list of urls(pages_list) and store them in a dataframe
```{r}


baseurl <- "https://www.theguardian.com/environment/cop26-glasgow-climate-change-conference-2021?page="
pageurl <- paste0(seq(1, 33, 1))
pages_list <- paste0(baseurl, pageurl)
pages_list[1:5] #33 pages until 18 oct = 2 weeks before cop --> I LOOKED AT IT BY HAND, IS THERE A WAY TO LOOK INTO IT AUTOMATICALLY? (NOT URGENT)
headlineslist <- c()
dateslist <- c()

#all_headlines = list()
for (page in pages_list){
  url_parsed <- read_html(page)
  
  headings_nodes <- html_elements(url_parsed, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "fc-date-headline", " " ))] | //*[contains(concat( " ", @class, " " ), concat( " ", "js-headline-text", " " ))]')
  
  headlines <- html_text2(headings_nodes)
  headlineslist <- append(headlineslist, headlines)
  #all_headlines <- append(all_headlines, headlineslist)
}

headlineslist <- unique(headlineslist)

# We iterate over the length of headlineslist, if we detect a match of "2021" in the headlineslist we recognise it as a date, and copy it into our dateslist. Note we keep the same position the date had in headlines. In the case were we don't find a date in the headlineslist, we fill the spot in dateslist with the previous entry (dateslist[i] <- dateslist[i-1]).

for (i in 1:length(headlineslist)){
  if(grepl("2021", headlineslist[i]) == TRUE){
    dateslist[i] <- headlineslist[i]
  }
  else if (grepl("2021", headlineslist[i]) == FALSE) {
    dateslist[i] <- dateslist[(i-1)]
  }
}

# First combining both our lists into a dataframe, such that each headline is associated with a date, and then removing all rows in which headlines = dates since we no longer need these for reference.  

cop26 <- cbind(as.data.frame(headlineslist), as.data.frame(dateslist)) %>% 
  filter(headlineslist != dateslist) %>% 
  rename("Headlines" = "headlineslist", "Dates" = "dateslist")

```
<br>

## Part 2 - Data cleaning 
```{r}

#formatting dates, will try to edit manually or import actual dates for the nonfomratted lines later
cop26$Dates<- as.Date(dmy(cop26$Dates) , format = "%d %B %Y")

customized_stopwords<- add_row(stop_words , word= c("cop26", "glasgow","climate","change")) #crisis

words_list<- cop26 %>% 
  unnest_tokens(word, Headlines, token = "regex", pattern = "\\s+|[[:punct:]]+") %>%
  anti_join(customized_stopwords) %>% 
  arrange(Dates)

```

## Explorative analysis

Through the wordcloud, we observed that words as "cop26", "glasgow","climate","change" were very frequent and not really useful for a sentiment analysis, so we removed them. 
```{r}
#simple frequency table
words_list %>% 
  count(word, sort = TRUE) 
 
#explorative wordcloud: we see the most frequent words: if there's something useless we remove it 
wordcloud(words_list$word, min.freq = 20, random.order = FALSE, colors=brewer.pal(8, "Dark2"))

#keyword in context
cop26$Headlines %>% 
  corpus() %>% 
  tokens() %>% 
  kwic(phrase("crisis"), window = 10) %>% 
  DT::datatable(caption = "Keywords in context", rownames = FALSE, options = list(scrollX = TRUE, pageLength = 5, lengthMenu = c(5, 10, 15, 20)))

#frequency plot + take most frequent words and see where they appear

```
## Sentiment analysis

Result: we have a dataframe with words, dates, sentiment and frequency of occurrence of those words
first plot: we start from the general: visualize through time how sentiment changes! 
```{r}
word_sent <- words_list %>% #we have 293 classified words
  filter(!word == "crisis") %>% #remove the word crisis before the sentiment analysis
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment) 
   
sentiment_complete <- left_join(words_list, word_sent, by = "word") #join with the main df

#word_sent %>% 
#  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) if we want to flip it 

sc <- sentiment_complete %>% #first plot of sentiment analysis
  drop_na()

sc$n[sc$sentiment == 'negative'] = -sc$n 


sc %>% 
    ggplot(aes(x = Dates, y = n, fill= sentiment))+
    geom_bar(position="dodge", stat="identity", width= 0.5) +
    labs(title = "Sentiment frequency plot by date", y = "Count", x = "Date", fill = "sentiment") +
    theme_minimal()

#add breaks
#add interactive slider for dates
#change color



```
## exploration of results

we try to understand which are the most used words overall. First we plot them without dates to see the overall result, then we add them in a second plot
```{r}
sc %>% 
  drop_na() %>% 
  count(word, sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>%
  filter(n > 4) %>% 
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```


# Targeted sentiment analysis for 'Boris Johnson' 
```{r}
#take 'boris' and 'johnson' words

toks_cop26 <- cop26 %>% 
 filter(str_detect(Headlines, "Boris Johnson")|str_detect(Headlines, "Boris") | str_detect(Headlines, "Johnson"))
  
boris_c26 <- toks_cop26$Headlines %>% 
  corpus() %>% 
  tokens()

sent_boris <- tokens_lookup(boris_c26, dictionary = data_dictionary_LSD2015[1:2]) %>% 
  dfm() %>% 
  convert(to = "data.frame") #targeted sentiment analysis for the boris words


sent_boris$score <- log((sent_boris$positive + 0.5) / (sent_boris$negative + 0.5))

sent_boris$average

sent_boris %>% 
  ggplot(aes(y = score)) + 
  geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.6) +
  coord_flip() +
  theme_minimal()

################################################################################

c26_density <- cop26$Headlines %>% 
  corpus() %>% 
  tokens()

sent_c26 <- tokens_lookup(c26_density, dictionary = data_dictionary_LSD2015[1:2]) %>% 
  dfm() %>% 
  convert(to = "data.frame") #targeted sentiment analysis for the boris words


sent_c26$score <- log((sent_c26$positive + 0.5) / (sent_c26$negative + 0.5))

sent_c26$average

sent_c26 %>% 
  ggplot(aes(y = score)) + 
  geom_density(fill="#69b3a2", color="#e9ecef", alpha=0.6) +
  coord_flip() +
  theme_minimal()

```

```{r}
# I kept these but we don't need them, 
#sent_negative <- words_list %>% #we have 293 classified words, DO WE NEED THESE? THE NEGATIVES ONLY
 # filter(!word == "crisis") %>% #remove the word crisis before the sentiment analysis
  #inner_join(get_sentiments("bing")) %>% 
  #filter(sentiment=="negative")

#sent_negative %>% 
 # count(Dates, sentiment) %>% 
  #ggplot(aes(x=Dates, y=n)) +
  #geom_line(color="red") +
  #theme_minimal() +
  #ylab("Frequency of Negative Words during COP26")+
  #xlab("Date")

#Interactive graph
sentiment_complete %>%
  count(Dates,sentiment) %>% 
  mutate(across(sentiment, .fns = ~replace_na(.,"neutral")))-> counter #creating a dataframe counting each type of sentiment, replacing NA sentiment by neutral
counter %>% 
  build_wider_spec(names_from = sentiment, values_from = c(n))->spec1
counter<-counter %>% 
  pivot_wider_spec(spec1) %>% 
  na.omit() # to pivot the data frame, single date per line, all sentiments per column 

negatives<-as.ts(counter$negative) #creating a time series for each sentiment
positives<-as.ts(counter$positive)
neutrals<-as.ts(counter$neutral)

timeseries <- cbind(negatives, positives,neutrals) # binding the three time series
dygraph(timeseries) %>%  #the plot, unfortunately, the dates are lost so far
  dySeries("negatives", label="Negative") %>% 
  dySeries("positives", label="Positive") %>% 
  dySeries("neutrals", label="Neutral") %>%
  dyOptions(labelsUTC = TRUE, fillGraph=TRUE, fillAlpha=0.5, drawGrid = FALSE) %>%
  dyRangeSelector() %>%
  dyCrosshair(direction = "vertical") %>%
  dyHighlight(highlightCircleSize = 3, highlightSeriesBackgroundAlpha = 0.3, hideOnMouseOut = FALSE)  %>%
  dyRoller(rollPeriod = 1)

#plotting positive and negative only

timeseries2<-cbind(negatives, positives)
dygraph(timeseries2) %>%  #the plot, unfortunately, the dates are lost so far
  dySeries("negatives", label="Negative") %>% 
  dySeries("positives", label="Positive") %>% 
  dyOptions(labelsUTC = TRUE, fillGraph=TRUE, fillAlpha=0.5, drawGrid = FALSE) %>%
  dyRangeSelector() %>%
  dyCrosshair(direction = "vertical") %>%
  dyHighlight(highlightCircleSize = 3, highlightSeriesBackgroundAlpha = 0.3, hideOnMouseOut = FALSE)  %>%
  dyRoller(rollPeriod = 1)


#I kept the ggplot on case we will drop the interactive ones
#sentiment_complete %>% 
 # drop_na() %>% 
  #count(Dates, sentiment) %>% 
  #group_by(sentiment) %>%
  #ggplot(aes(Dates,y=n,color=sentiment)) +
  #geom_line() +
  #facet_wrap(~ sentiment, scales = "free_x") +
  #theme_minimal() +
  #ylab("Frequency of Negative Words during COP26")+
  #xlab("Date")

```


```{r}
#FOR OTHER IDEAS

#https://www.tidytextmining.com/ngrams.html
```


