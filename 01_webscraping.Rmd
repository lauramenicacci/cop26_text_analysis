---
title: "01_downloading articles"
author: "Laura Menicacci"
date: "28/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages

```{r}
library(tidyverse)
library(rvest)
library(stringr)
library(xml2)
library(quanteda)
library(tidytext)
library(lubridate)
library(wordcloud)
library(SnowballC)
```
<br>

## Part 1 - Scraping headlines by date 

what i did until now: 
 - create a list of all guardian pages within the chosen period (18 octr to 28 nov) (RMK:first/last page is with dates also before after 28 nov/18 oct but we can remove them in the cleaning part)
 - try to download pages from list of urls(pages_list) and store them in a dataframe
```{r}

baseurl <- "https://www.theguardian.com/environment/cop26-glasgow-climate-change-conference-2021?page="
pageurl <- paste0(seq(1, 33, 1))
pages_list <- paste0(baseurl, pageurl)
pages_list[1:5] #33 pages until 18 oct = 2 weeks before cop --> I LOOKED AT IT BY HAND, IS THERE A WAY TO LOOK INTO IT AUTOMATICALLY? (NOT URGENT)
headlineslist <- c()
dateslist <- c()

#all_headlines = list()
for (page in pages_list){
  url_parsed <- read_html(page)
  
  headings_nodes <- html_elements(url_parsed, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "fc-date-headline", " " ))] | //*[contains(concat( " ", @class, " " ), concat( " ", "js-headline-text", " " ))]')
  
  headlines <- html_text2(headings_nodes)
  headlineslist <- append(headlineslist, headlines)
  #all_headlines <- append(all_headlines, headlineslist)
}

headlineslist <- unique(headlineslist)

# We iterate over the length of headlineslist, if we detect a match of "2021" in the headlineslist we recognise it as a date, and copy it into our dateslist. Note we keep the same position the date had in headlines. In the case were we don't find a date in the headlineslist, we fill the spot in dateslist with the previous entry (dateslist[i] <- dateslist[i-1]).

for (i in 1:length(headlineslist)){
  if(grepl("2021", headlineslist[i]) == TRUE){
    dateslist[i] <- headlineslist[i]
  }
  else if (grepl("2021", headlineslist[i]) == FALSE) {
    dateslist[i] <- dateslist[(i-1)]
  }
}

# First combining both our lists into a dataframe, such that each headline is associated with a date, and then removing all rows in which headlines = dates since we no longer need these for reference.  

cop26 <- cbind(as.data.frame(headlineslist), as.data.frame(dateslist)) %>% 
  filter(headlineslist != dateslist) %>% 
  rename("Headlines" = "headlineslist", "Dates" = "dateslist")

```
<br>

## Part 2 - Data cleaning 
```{r}

#formatting dates, will try to edit manually or import actual dates for the nonfomratted lines later
cop26$Dates<- as.Date(dmy(cop26$Dates) , format = "%d %B %Y")

customized_stopwords<- add_row(stop_words , word= c("cop26", "glasgow","climate","change")) #crisis

words_list<- cop26 %>% 
  unnest_tokens(word, Headlines, token = "regex", pattern = "\\s+|[[:punct:]]+") %>%
  anti_join(customized_stopwords) %>% 
  arrange(Dates)

```

## Explorative analysis

Through the wordcloud, we observed that words as "cop26", "glasgow","climate","change" were very frequent and not really useful for a sentiment analysis, so we removed them. 
```{r}
#simple frequency table
words_list %>% 
  count(word, sort = TRUE) 
 
#explorative wordcloud: we see the most frequent words: if there's something useless we remove it 
wordcloud(words_list$word, min.freq = 20, random.order = FALSE, colors=brewer.pal(8, "Dark2"))

#keyword in context
cop26$Headlines %>% 
  corpus() %>% 
  tokens() %>% 
  kwic(phrase("crisis"), window = 10) %>% 
  DT::datatable(caption = "Keywords in context", rownames = FALSE, options = list(scrollX = TRUE, pageLength = 5, lengthMenu = c(5, 10, 15, 20)))

```
## Sentiment analysis

Result: we have a dataframe with words, dates, sentiment and frequency of occurrence of those words
```{r}
word_sent <- words_list %>% 
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment) 

sentiment_complete <- left_join(words_list, word_sent, by = "word")

#TO DO: new df w pos and neg counts as columns 

sentiment_complete %>% 
  drop_na() %>% 
  ggplot(aes(x = Dates, y = n, fill= -1*(sentiment== "negative")))+
  geom_bar(position="dodge", stat="identity", width= 0.5) +
  labs(title = "Sentiment frequency plot by date", y = "Frequency", x = "Date", fill = "sentiment") +
  theme_minimal()

```

#Targeted sentiment analysis
```{r}
#find most frequent neg/pos words

#cop26$Headlines %>% 
#  corpus() %>% 
#  tokens() %>%
#  tokens_select("WORD", selection = "keep", window = 20, padding = FALSE, verbose = TRUE)
#  tokens_lookup(data_dictionary_LSD2015[1:2])


```





