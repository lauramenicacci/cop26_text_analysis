---
title: "A Sentiment Analysis of the Headlines of COP26 on 'The Guardian' "
author: "Katalin Bayer, Laura Menicacci, Nassim Zoueini"
date: "19/12/2021"
output: html_document
---

<style>
div.comment {font-size:14pt; background-color:#f3f0ff; border-radius: 5px; padding: 20px;}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<br>

<div class="comment">
<strong> Introduction </strong>
<br>
In this project, we target the headlines of "The Guardian" newspaper between 18. October 2021 and 28. November 2021 to apply a sentiment analysis around the negotiations of COP26 that took place in Glasgow from October 31st and November 12. 
<br>
Being a newspaper of the host country of the climate negotiation, the guardian would not present an ideal sample of headlines that would allow us to deduce  if COP26 has met the expectations or not through the sentiment analysis. However, the values of this project is to apply procedures of sentiment analysis after scraping information from the web and present them to the user in an accessible format.
</div>

<br>
<h2>
1. Packages
</h2>

```{r}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, 
               rvest, 
               stringr, 
               xml2, 
               quanteda, 
               janitor, 
               tidytext, 
               lubridate, 
               wordcloud, 
               SnowballC, 
               dygraphs, 
               xts, 
               tsbox,
               tm)

# DO WE NEED TO LOAD TM SEPERATELY OR NOT?
```
<br>

<h2>
2. Scraping Headlines by Date and Data Cleaning
</h2>
<div>
<i> To examine this step in details, check the attached scraping.R file on <a href="url"> https://github.com/lauramenicacci/cop26_text_analysis/blob/e3697a69534a5e9d540bfc1913f52bd096de99ee/scraping.R</a> </i>
</div>
```{r}
source("./scraping_and_data_cleaning.R")

```

<br>
<br>
<h2>
3. Explorative analysis
</h2>
<div class="comment">
Through the wordcloud, we observed that words as "cop26", "glasgow","climate","change" were very frequent and not really useful for a sentiment analysis, so we removed them.
PLEASE LAURA ADD OTHER RELEVANT DESCRIPTION HERE
3.1. Frequency Table 
<br>
3.2. Explorative Wordcloud
<br>
3.3.Keyword in Context
<br>
</div>
<br>
<br>
<h3>
3.1. Frequency Table
</h3>
```{r}
words_list %>% 
  count(word, sort = TRUE) %>% 
  head(10)
 

#frequency plot + take most frequent words and see where they appear 
#WE DROPPED THIS?

```
<br>
<h3>
3.2. Explorative Wordcloud
</h3>
```{r}
#explorative wordcloud: we see the most frequent words: if there's something useless we remove it 
wordcloud(words_list$word, min.freq = 20, random.order = FALSE, colors=brewer.pal(8, "Dark2"))
```
<div class="comment">
<i> Exploring the most frequent words helps to remove the irrelevant ones among them by adding them to the stopwords dictionary i.e. Glasgow. EDIT PLEASE </i>
<br>
</div>
<div>
<i>Check scraping file https://github.com/lauramenicacci/cop26_text_analysis/blob/e3697a69534a5e9d540bfc1913f52bd096de99ee/scraping.R</a>
</i>
<br>
</div>
<br>
<h3>
3.3. Keyword in Context
</h3>
```{r}
cop26$Headlines %>% 
  corpus() %>% 
  tokens() %>% 
  kwic(phrase("crisis"), window = 10) %>% 
  DT::datatable(caption = "Keywords in context", rownames = FALSE, options = list(scrollX = TRUE, pageLength = 5, lengthMenu = c(5, 10, 15, 20)))
```
<br>
<br>
<h2>
4. Sentiment analysis Using "bing" Dictionary
</h2>
<br>
<div class="comment">
Result: we have a dataframe with words, dates, sentiment and frequency of occurrence of those words
first plot: we start from the general: visualize through time how sentiment changes! TO BE EDITED

<br>
4.1. Sentiment Analysis Dataframe
<br>
4.2. Basic Visualization
</div>
<br>
<h3>
4.1. Sentiment Analysis Dataframe
</h3>
```{r}
word_sent <- words_list %>% #we have 293 classified words
  filter(!word == "crisis") %>% #remove the word crisis before the sentiment analysis
  inner_join(get_sentiments("bing")) %>% 
  count(word, sentiment) 
   
sentiment_complete <- left_join(words_list, word_sent, by = "word") #join with the main df

#word_sent %>% 
#  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) if we want to flip it 


toks_cop26 <- cop26 %>% 
 filter(str_detect(Headlines, "Boris Johnson")|str_detect(Headlines, "Boris") | str_detect(Headlines, "Johnson"))

words_list_boris<- toks_cop26 %>% 
  unnest_tokens(word, Headlines, token = "regex", pattern = "\\s+|[[:punct:]]+") %>%
  anti_join(customized_stopwords) %>% 
  arrange(Dates) %>%
  count(word, sort = TRUE) %>% 
  head(10)
 

```
<br>
<h3>
4.2. Basic Visualization
</h3>

```{r}
sc <- sentiment_complete %>% #first plot of sentiment analysis
  drop_na()

sc$n[sc$sentiment == 'negative'] <-  -sc$n 


sc %>%
  ggplot(aes(x = Dates, y = n, fill = sentiment))+
  geom_bar(stat = "identity", width = 0.5) +
  labs(title = "Sentiment frequency plot by date", y = "Count", x = "Date", fill = "sentiment") + geom_vline(xintercept= as.Date("2021-10-31"), linetype = "dashed", colour = "darkgrey") +
  geom_vline(xintercept= as.Date("2021-11-12"), linetype = "dashed", colour = "darkgrey") +
  theme_minimal()+ theme(panel.border = element_rect(color = "black",
                                    fill = NA,
                                    size = 0.3))
#ggsave("SentimentFrequencyPlot.png")
```

<br>
<br>

<h2>
5. Exploration of Results
</h2>
<br>
<div class="comment">
PLEASE LAURA ADD DESCRIPTION HERE

we try to understand which are the most used words overall. First we plot them without dates to see the overall result, then we add them in a second plot
<br>
5.1. Plot of Results
<br>
5.2. Summary Interactive Plot by Date
<br>
5.2.1. Manipulation of the Dataset into a #timeseries' Object
<br>
5.2.2. All Sentiment Interactive Plot by Date
<br>
5.2.3. Positive vs Negative Sentiment Interactive Plots by Date
</div>
<br>
<h3>
5.1. Plot of Results
</h3>
```{r}
sc %>% 
  drop_na() %>% 
  count(word, sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>%
  filter(n > 4) %>% 
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ sentiment, scales = "free_y") +
  labs(title = "Most common words affecting Sentiments", x = "Frequency", y = NULL)+
  theme_minimal() + 
  theme(panel.border = element_rect(color = "black",
                                    fill = NA,
                                    size = 0.3))
ggsave("MostFrequentlyOccuringWord.png")
```
<br>
<h3>
5.2. Summary Interactive Plots
</h3>
<h4>
5.2.1. Manipulation of the Dataset into a 'timeseries' Object
</h4>
```{r}

sentiment_complete %>%
  count(Dates,sentiment) %>% 
  mutate(across(sentiment, .fns = ~replace_na(.,"neutral")))-> counter #creating a dataframe counting each type of sentiment, replacing NA sentiment by neutral

counter %>% 
  build_wider_spec(names_from = sentiment, values_from = c(n))->spec1
counter<-counter %>% 
  pivot_wider_spec(spec1) %>% 
  na.omit() # to pivot the data frame, single date per line, all sentiments per column 

negatives<-xts(counter$negative, counter$Dates) #creating a time series for each sentiment
positives<-xts(counter$positive, counter$Dates)
neutrals<-xts(counter$neutral, counter$Dates)

timeseries <- cbind(negatives, positives,neutrals) # binding the three time series

```
<br>
<h4>
5.2.2. All Sentiment Interactive Plot by Date
</h4>
```{r}
dygraph(timeseries, main = "All sentiments across time") %>%  #the plot, unfortunately, the dates are lost so far
  dySeries("negatives", label="Negative") %>% 
  dySeries("positives", label="Positive") %>% 
  dySeries("neutrals", label="Neutral") %>%
  dyOptions(labelsUTC = TRUE, fillGraph=TRUE, fillAlpha=0.5, drawGrid = FALSE) %>%
  dyRangeSelector() %>%
  dyCrosshair(direction = "vertical") %>%
  dyHighlight(highlightCircleSize = 3, highlightSeriesBackgroundAlpha = 0.3, hideOnMouseOut = FALSE)  %>%
  dyRoller(rollPeriod = 1) 
```
<br>
<h4>
5.2.3. Positive vs Negative Sentiment Interactive Plots by Date
</h4>
```{r}

timeseries2<-cbind(negatives, positives)
dygraph(timeseries2) %>%  #the plot, unfortunately, the dates are lost so far
  dySeries("negatives", label="Negative") %>% 
  dySeries("positives", label="Positive") %>% 
  dyOptions(labelsUTC = TRUE, fillGraph=TRUE, fillAlpha=0.5, drawGrid = FALSE) %>%
  dyRangeSelector() %>%
  dyCrosshair(direction = "vertical") %>%
  dyHighlight(highlightCircleSize = 3, highlightSeriesBackgroundAlpha = 0.3, hideOnMouseOut = FALSE)  %>%
  dyRoller(rollPeriod = 1)
```

<br>
<br>
<h2>
6. Targeted sentiment analysis
</h2>
<div class="comment">
KATALIN PLEASE EDIT HERE
<br>
6.1 Sentiment Density for COP26
<br>
6.2 Sentiment Density, Target: Boris Johnson
</div>
<br>
<h3>
6.1 Sentiment Density for COP26
</h3>
```{r}

toks_c26 <- cop26$Headlines %>% 
  corpus() %>% 
  tokens()
boris_c26 <- toks_cop26$Headlines %>% 
  corpus() %>% 
  tokens()

sent_c26 <- tokens_lookup(toks_c26, dictionary = data_dictionary_LSD2015[1:2]) %>% 
  dfm() %>% 
  convert(to = "data.frame") %>% #targeted sentiment analysis for the toks_cop26 words
  mutate(Type = "Cop26")
  
sent_boris <- tokens_lookup(boris_c26, dictionary = data_dictionary_LSD2015[1:2]) %>% 
  dfm() %>% 
  convert(to = "data.frame") %>% #targeted sentiment analysis for the toks_cop26 words
  mutate(Type = "Boris")

sentiment <- rbind(sent_c26,sent_boris)
sentiment$score <- log((sentiment$positive + 0.5) / (sentiment$negative + 0.5))

sentiment %>% 
  ggplot(aes(x = score, fill = Type)) + 
  geom_density(alpha=0.3) +
  labs(title = "Distribution of Sentiments", subtitle = str_wrap('This graph plots the density of sentiments across all headlines, as well as the density of a targeted sentiment analysis around the name "Boris Johnson".', width = 85), x = "Sentiment Score", y = "Density")+
  theme_minimal() + 
  theme_minimal() +
  theme(panel.border = element_rect(color = "black",
                                    fill = NA,
                                    size = 0.3))

```

```{r}
# I kept these but we don't need them, 
#sent_negative <- words_list %>% #we have 293 classified words, DO WE NEED THESE? THE NEGATIVES ONLY
 # filter(!word == "crisis") %>% #remove the word crisis before the sentiment analysis
  #inner_join(get_sentiments("bing")) %>% 
  #filter(sentiment=="negative")

#sent_negative %>% 
 # count(Dates, sentiment) %>% 
  #ggplot(aes(x=Dates, y=n)) +
  #geom_line(color="red") +
  #theme_minimal() +
  #ylab("Frequency of Negative Words during COP26")+
  #xlab("Date")




#I kept the ggplot on case we will drop the interactive ones
#sentiment_complete %>% 
 # drop_na() %>% 
  #count(Dates, sentiment) %>% 
  #group_by(sentiment) %>%
  #ggplot(aes(Dates,y=n,color=sentiment)) +
  #geom_line() +
  #facet_wrap(~ sentiment, scales = "free_x") +
  #theme_minimal() +
  #ylab("Frequency of Negative Words during COP26")+
  #xlab("Date")

```


```{r}
#FOR OTHER IDEAS

#https://www.tidytextmining.com/ngrams.html
```


